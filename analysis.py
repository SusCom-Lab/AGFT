#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
vLLM GPU Autoscaler æ•°æ®åˆ†æå·¥å…· - é’ˆå¯¹æ–°JSONæ—¥å¿—æ ¼å¼é‡æ„
Refactored Data Analysis Tool for New JSON Log Format

æ­¤å·¥å…·ä¸“é—¨é’ˆå¯¹æ–°çš„JSONç»“æ„åŒ–æ—¥å¿—æ ¼å¼è®¾è®¡ï¼Œæä¾›å…¨é¢çš„åœ¨çº¿å­¦ä¹ æ•ˆæœè¯„ä¼°ã€‚
"""

import os
import re
import json
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
from glob import glob
from typing import Dict, List, Tuple, Optional, Any
import warnings
warnings.filterwarnings('ignore')

# Set matplotlib environment
os.environ['MPLCONFIGDIR'] = '/tmp/matplotlib_cache'
os.makedirs('/tmp/matplotlib_cache', exist_ok=True)

import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
from collections import Counter, defaultdict

class NumpyEncoder(json.JSONEncoder):
    """JSON encoder for NumPy types"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif hasattr(obj, 'isoformat'):
            return obj.isoformat()
        return super().default(obj)

# Setup matplotlib with font configuration
def setup_display():
    """Setup matplotlib with font configuration"""
    try:
        user_font_path = '/home/ldaphome/colin/.local/share/fonts/NotoSansCJKsc-Regular.otf'
        if os.path.exists(user_font_path):
            fm.fontManager.addfont(user_font_path)
            plt.rcParams['font.family'] = ['Noto Sans CJK SC', 'sans-serif']
        else:
            plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
    except:
        plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
    
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['figure.max_open_warning'] = 0
    plt.rcParams['figure.figsize'] = (12, 8)
    plt.rcParams['figure.dpi'] = 100
    
    try:
        plt.style.use('seaborn-v0_8')
    except:
        try:
            plt.style.use('seaborn')
        except:
            plt.style.use('default')

setup_display()

class VLLMAutoscalerAnalyzer:
    """vLLM GPU Autoscaler æ•°æ®åˆ†æå™¨ - æ–°JSONæ ¼å¼"""
    
    def __init__(self, log_dir: str = "logs", output_dir: str = "data/analysis"):
        self.log_dir = log_dir
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"ğŸ” åˆå§‹åŒ–æ•°æ®åˆ†æå™¨ (JSONæ ¼å¼)...")
        print(f"   æ—¥å¿—ç›®å½•: {log_dir}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    
    def find_latest_log(self, gpu_model: str = None) -> Optional[str]:
        """æŸ¥æ‰¾æœ€æ–°çš„æ—¥å¿—æ–‡ä»¶"""
        if gpu_model:
            gpu_log_dir = os.path.join(self.log_dir, gpu_model)
            if os.path.exists(gpu_log_dir):
                log_files = sorted(glob(os.path.join(gpu_log_dir, "*.log")))
            else:
                print(f"âš ï¸ GPUç›®å½•ä¸å­˜åœ¨: {gpu_log_dir}")
                return None
        else:
            log_files = []
            gpu_dirs = [d for d in os.listdir(self.log_dir) 
                       if os.path.isdir(os.path.join(self.log_dir, d)) and not d.startswith('.')]
            
            for gpu_dir in gpu_dirs:
                gpu_log_dir = os.path.join(self.log_dir, gpu_dir)
                gpu_logs = glob(os.path.join(gpu_log_dir, "*.log"))
                log_files.extend(gpu_logs)
        
        if not log_files:
            print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
            return None
        
        return max(log_files, key=os.path.getmtime)
    
    def parse_log(self, log_file_path: str) -> pd.DataFrame:
        """è§£ææ–°JSONæ ¼å¼çš„æ—¥å¿—æ–‡ä»¶"""
        print(f"ğŸ“– è§£æJSONæ ¼å¼æ—¥å¿—æ–‡ä»¶: {os.path.basename(log_file_path)}")
        
        if not os.path.exists(log_file_path):
            print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
            return pd.DataFrame()
        
        parsed_data = []
        system_info = {}
        mode_switch_events = []  # è®°å½•æ¨¡å¼åˆ‡æ¢äº‹ä»¶
        
        # JSONæ•°æ®æå–æ¨¡å¼
        json_pattern = r'ğŸ“‹ JSONæ•°æ®: ({.*})'
        
        # ç³»ç»Ÿä¿¡æ¯æå–æ¨¡å¼
        info_patterns = {
            'gpu_model': r'ğŸ”§ GPUå‹å·: (.+)',
            'algorithm': r'ç®—æ³•: ([^(]+)',
            'feature_dims': r'ç‰¹å¾ç»´åº¦: (\d+)ç»´',
            'convergence_window': r'æ”¶æ•›çª—å£: (\d+)è½®'
        }
        
        # æ–°å¢æ¨¡å¼åˆ‡æ¢æ£€æµ‹æ¨¡å¼
        mode_switch_patterns = {
            'convergence_detected': r'ğŸ¯ æ¨¡å‹æ”¶æ•›æ£€æµ‹: ä¸»å¯¼åŠ¨ä½œ(\d+)MHzå æ¯”([\d.]+)% >= (\d+)%',
            'switch_to_exploitation': r'ğŸ”„ ä»å­¦ä¹ æ¨¡å¼åˆ‡æ¢åˆ°åˆ©ç”¨æ¨¡å¼',
            'performance_degradation': r'âš ï¸ æ€§èƒ½é€€åŒ–æ£€æµ‹: æœ€è¿‘50è½®EDPå¹³å‡å€¼\(([\d.]+)\)',
            'switch_to_learning': r'ğŸ”„ (.+)ï¼Œä»åˆ©ç”¨æ¨¡å¼åˆ‡æ¢å›å­¦ä¹ æ¨¡å¼',
            'adaptive_sampler_skip': r'ğŸ”’ åˆ©ç”¨æ¨¡å¼ä¸‹è·³è¿‡è‡ªé€‚åº”é‡‡æ ·å™¨æ›´æ–°',
            'exploitation_to_learning': r'âš ï¸.*ä»åˆ©ç”¨æ¨¡å¼åˆ‡æ¢å›å­¦ä¹ æ¨¡å¼',
            'convergence_unstable': r'âš ï¸ æ”¶æ•›çŠ¶æ€ä¸ç¨³å®š.*ä¸»å¯¼åŠ¨ä½œå æ¯”é™è‡³([\d.]+)%'
        }
        
        try:
            with open(log_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–ç³»ç»Ÿä¿¡æ¯
            for key, pattern in info_patterns.items():
                match = re.search(pattern, content)
                if match:
                    if key == 'feature_dims' or key == 'convergence_window':
                        system_info[key] = int(match.group(1))
                    else:
                        system_info[key] = match.group(1).strip()
            
            # æå–æ¨¡å¼åˆ‡æ¢äº‹ä»¶
            for event_type, pattern in mode_switch_patterns.items():
                for line in content.split('\n'):
                    if re.search(pattern, line):
                        # æå–æ—¶é—´æˆ³
                        timestamp_match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                        if timestamp_match:
                            event_info = {
                                'timestamp': pd.to_datetime(timestamp_match.group(1)),
                                'event_type': event_type,
                                'description': line.strip()
                            }
                            mode_switch_events.append(event_info)
            
            print(f"   æ£€æµ‹åˆ° {len(mode_switch_events)} ä¸ªæ¨¡å¼åˆ‡æ¢äº‹ä»¶")
            
            # æå–æ‰€æœ‰JSONæ•°æ®
            json_matches = re.findall(json_pattern, content)
            print(f"   æ‰¾åˆ° {len(json_matches)} æ¡JSONè®°å½•")
            
            for json_str in json_matches:
                try:
                    # è§£æJSONæ•°æ®
                    json_data = json.loads(json_str)
                    
                    # æ„é€ æ‰å¹³åŒ–çš„æ•°æ®æ¡ç›®
                    data_entry = {
                        # åŸºç¡€ä¿¡æ¯
                        'timestamp': pd.to_datetime(json_data.get('timestamp')),
                        'round': json_data.get('round'),
                        
                        # å†³ç­–ä¿¡æ¯
                        'selected_frequency': json_data.get('decision', {}).get('selected_frequency'),
                        'selection_count': json_data.get('decision', {}).get('selection_count'),
                        'mode': json_data.get('decision', {}).get('mode'),
                        'available_frequencies': json_data.get('decision', {}).get('available_frequencies', []),
                        'n_available_frequencies': len(json_data.get('decision', {}).get('available_frequencies', [])),
                        
                        # ç‰¹å¾å‘é‡ (7ç»´)
                        'has_queue': json_data.get('features', {}).get('has_queue'),
                        'prefill_throughput': json_data.get('features', {}).get('prefill_throughput'),
                        'decode_throughput': json_data.get('features', {}).get('decode_throughput'),
                        'packing_efficiency': json_data.get('features', {}).get('packing_efficiency'),
                        'concurrency': json_data.get('features', {}).get('concurrency'),
                        'gpu_cache_usage': json_data.get('features', {}).get('gpu_cache_usage'),
                        'cache_hit_rate': json_data.get('features', {}).get('cache_hit_rate'),
                        
                        # æ€§èƒ½æŒ‡æ ‡
                        'avg_ttft': json_data.get('performance', {}).get('avg_ttft'),
                        'avg_tpot': json_data.get('performance', {}).get('avg_tpot'),
                        'ttft_count': json_data.get('performance', {}).get('ttft_count'),
                        'tpot_count': json_data.get('performance', {}).get('tpot_count'),
                        'energy_delta_mj': json_data.get('performance', {}).get('energy_delta_mj'),
                        'energy_delta_j': json_data.get('performance', {}).get('energy_delta_j'),
                        'edp_value': json_data.get('performance', {}).get('edp_value'),
                        
                        # å¥–åŠ±ä¿¡æ¯
                        'current_reward': json_data.get('reward', {}).get('current'),
                        'average_reward': json_data.get('reward', {}).get('average'),
                        'recent_average_reward': json_data.get('reward', {}).get('recent_average'),
                        
                        # ç³»ç»ŸçŠ¶æ€
                        'gpu_utilization': json_data.get('system', {}).get('gpu_utilization'),
                        'gpu_memory_mb': json_data.get('system', {}).get('gpu_memory_mb'),
                        'queue_size': json_data.get('system', {}).get('queue_size'),
                        
                        # æ¨¡å‹çŠ¶æ€
                        'total_rounds': json_data.get('model', {}).get('total_rounds'),
                        'n_arms': json_data.get('model', {}).get('n_arms'),
                        'converged': json_data.get('model', {}).get('converged'),
                        'exploitation_mode': json_data.get('model', {}).get('exploitation_mode'),
                        
                        # æ·»åŠ ç³»ç»Ÿä¿¡æ¯
                        **system_info
                    }
                    
                    # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
                    # å¤„ç†æ–°çš„æ¨¡å¼å­—æ®µ
                    mode = json_data.get('decision', {}).get('mode', 'exploration')
                    if mode == 'exploitation' or data_entry['exploitation_mode']:
                        data_entry['phase'] = 'EXPLOITATION'
                    else:
                        data_entry['phase'] = 'LEARNING'
                    data_entry['energy_efficiency'] = 1.0 / max(data_entry['edp_value'], 1e-6) if data_entry['edp_value'] else 0
                    data_entry['total_latency'] = (data_entry['avg_ttft'] or 0) + (data_entry['avg_tpot'] or 0)
                    data_entry['throughput_ratio'] = (data_entry['decode_throughput'] or 0) / max(data_entry['prefill_throughput'] or 1, 1e-6)
                    
                    parsed_data.append(data_entry)
                    
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                    continue
                except Exception as e:
                    print(f"âš ï¸ æ•°æ®å¤„ç†å¤±è´¥: {e}")
                    continue
        
        except Exception as e:
            print(f"âŒ è§£ææ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            return pd.DataFrame()
        
        if not parsed_data:
            print("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONæ•°æ®")
            return pd.DataFrame()
        
        df = pd.DataFrame(parsed_data)
        
        # å­˜å‚¨æ¨¡å¼åˆ‡æ¢äº‹ä»¶ä¿¡æ¯
        self.mode_switch_events = mode_switch_events
        
        # æ•°æ®æ¸…ç†å’Œæ’åº
        df = df.sort_values('round').reset_index(drop=True)
        
        print(f"   è§£æå®Œæˆ: {len(df)} æ¡è®°å½•")
        print(f"   æ—¶é—´èŒƒå›´: {df['timestamp'].min()} è‡³ {df['timestamp'].max()}")
        print(f"   è½®æ¬¡èŒƒå›´: {df['round'].min()} - {df['round'].max()}")
        print(f"   ç³»ç»Ÿä¿¡æ¯: {system_info}")
        
        return df
    
    def analyze_learning_effectiveness(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æåœ¨çº¿å­¦ä¹ æ•ˆæœ - æ ¸å¿ƒåˆ†ææ–¹æ³•"""
        print("ğŸ§  åˆ†æåœ¨çº¿å­¦ä¹ æ•ˆæœ...")
        
        if df.empty:
            return {}
        
        analysis = {}
        
        # 1. åŸºç¡€ç»Ÿè®¡
        analysis['basic_stats'] = {
            'total_rounds': len(df),
            'learning_rounds': len(df[df['phase'] == 'LEARNING']),
            'exploitation_rounds': len(df[df['phase'] == 'EXPLOITATION']),
            'unique_frequencies_tried': df['selected_frequency'].nunique(),
            'total_frequencies_available': df['n_available_frequencies'].iloc[-1] if not df.empty else 0,
            'algorithm': df['algorithm'].iloc[0] if 'algorithm' in df.columns else 'Unknown',
            'gpu_model': df['gpu_model'].iloc[0] if 'gpu_model' in df.columns else 'Unknown'
        }
        
        # 2. æ”¶æ•›åˆ†æ
        analysis['convergence_analysis'] = self._analyze_convergence(df)
        
        # 3. æ¢ç´¢-åˆ©ç”¨æƒè¡¡åˆ†æ
        analysis['exploration_exploitation'] = self._analyze_exploration_exploitation(df)
        
        # 4. å¥–åŠ±å­¦ä¹ åˆ†æ
        analysis['reward_learning'] = self._analyze_reward_learning(df)
        
        # 5. é¢‘ç‡åå¥½å­¦ä¹ åˆ†æ
        analysis['frequency_preference'] = self._analyze_frequency_preference(df)
        
        # 6. ç‰¹å¾é‡è¦æ€§åˆ†æ
        analysis['feature_analysis'] = self._analyze_feature_patterns(df)
        
        # 7. æ€§èƒ½ä¼˜åŒ–æ•ˆæœåˆ†æ
        analysis['performance_optimization'] = self._analyze_performance_optimization(df)
        
        # 8. èƒ½æ•ˆåˆ†æ
        analysis['energy_efficiency'] = self._analyze_energy_efficiency(df)
        
        # 9. è‡ªé€‚åº”èƒ½åŠ›åˆ†æ
        analysis['adaptability'] = self._analyze_adaptability(df)
        
        # 10. åœ¨çº¿å­¦ä¹ è´¨é‡è¯„ä¼°
        analysis['learning_quality'] = self._evaluate_learning_quality(df)
        
        # 11. æ¨¡å¼åˆ‡æ¢äº‹ä»¶åˆ†æ
        analysis['mode_switching'] = self._analyze_mode_switching_events(df)
        
        return analysis
    
    def _analyze_convergence(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ”¶æ•›è¡Œä¸º"""
        convergence = {}
        
        # æ”¶æ•›æ£€æµ‹
        learning_df = df[df['phase'] == 'LEARNING']
        exploitation_df = df[df['phase'] == 'EXPLOITATION']
        
        convergence['convergence_achieved'] = len(exploitation_df) > 0
        convergence['rounds_to_convergence'] = len(learning_df) if convergence['convergence_achieved'] else None
        convergence['convergence_ratio'] = len(exploitation_df) / len(df) * 100 if len(df) > 0 else 0
        
        # æ”¶æ•›ç¨³å®šæ€§åˆ†æ
        if len(exploitation_df) > 0:
            # åˆ©ç”¨é˜¶æ®µçš„é¢‘ç‡ç¨³å®šæ€§
            exploit_freqs = exploitation_df['selected_frequency'].value_counts()
            convergence['dominant_frequency'] = exploit_freqs.index[0] if not exploit_freqs.empty else None
            convergence['frequency_stability'] = exploit_freqs.iloc[0] / len(exploitation_df) * 100 if not exploit_freqs.empty else 0
            
            # å¥–åŠ±ç¨³å®šæ€§
            exploit_rewards = exploitation_df['current_reward']
            convergence['reward_stability'] = {
                'mean': exploit_rewards.mean(),
                'std': exploit_rewards.std(),
                'coefficient_of_variation': exploit_rewards.std() / max(abs(exploit_rewards.mean()), 1e-6)
            }
        
        return convergence
    
    def _analyze_exploration_exploitation(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ¢ç´¢-åˆ©ç”¨æƒè¡¡"""
        exp_exp = {}
        
        # é¢‘ç‡é€‰æ‹©å¤šæ ·æ€§åˆ†æ
        freq_counts = df['selected_frequency'].value_counts()
        total_selections = len(df)
        
        exp_exp['frequency_diversity'] = {
            'unique_frequencies': len(freq_counts),
            'gini_coefficient': self._calculate_gini_coefficient(freq_counts.values),
            'entropy': self._calculate_entropy(freq_counts.values),
            'most_selected_frequency': freq_counts.index[0] if not freq_counts.empty else None,
            'most_selected_ratio': freq_counts.iloc[0] / total_selections * 100 if not freq_counts.empty else 0
        }
        
        # æ¢ç´¢æ¨¡å¼åˆ†æ
        learning_df = df[df['phase'] == 'LEARNING']
        if not learning_df.empty:
            learning_freq_counts = learning_df['selected_frequency'].value_counts()
            exp_exp['exploration_pattern'] = {
                'frequencies_explored': len(learning_freq_counts),
                'exploration_uniformity': 1.0 - self._calculate_gini_coefficient(learning_freq_counts.values),
                'average_explorations_per_frequency': learning_freq_counts.mean()
            }
        
        # åˆ©ç”¨æ¨¡å¼åˆ†æ
        exploitation_df = df[df['phase'] == 'EXPLOITATION']
        if not exploitation_df.empty:
            exploit_freq_counts = exploitation_df['selected_frequency'].value_counts()
            exp_exp['exploitation_pattern'] = {
                'exploitation_concentration': exploit_freq_counts.iloc[0] / len(exploitation_df) * 100 if not exploit_freq_counts.empty else 0,
                'frequencies_used_in_exploitation': len(exploit_freq_counts)
            }
        
        return exp_exp
    
    def _analyze_reward_learning(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æå¥–åŠ±å­¦ä¹ æ•ˆæœ"""
        reward_learning = {}
        
        if 'current_reward' in df.columns:
            rewards = df['current_reward'].dropna()
            
            # å¥–åŠ±è¶‹åŠ¿åˆ†æ
            reward_learning['trend_analysis'] = {
                'initial_reward': rewards.iloc[0] if not rewards.empty else None,
                'final_reward': rewards.iloc[-1] if not rewards.empty else None,
                'reward_improvement': rewards.iloc[-1] - rewards.iloc[0] if len(rewards) > 0 else 0,
                'average_reward': rewards.mean(),
                'reward_volatility': rewards.std()
            }
            
            # å­¦ä¹ é€Ÿåº¦åˆ†æ
            if len(rewards) > 10:
                # è®¡ç®—ç§»åŠ¨å¹³å‡æ¥è¯„ä¼°å­¦ä¹ é€Ÿåº¦
                window_size = min(20, len(rewards) // 4)
                moving_avg = rewards.rolling(window=window_size).mean()
                reward_learning['learning_speed'] = {
                    'window_size': window_size,
                    'early_avg': moving_avg.iloc[window_size-1] if len(moving_avg) >= window_size else None,
                    'late_avg': moving_avg.iloc[-1] if not moving_avg.empty else None,
                    'improvement_rate': (moving_avg.iloc[-1] - moving_avg.iloc[window_size-1]) / window_size if len(moving_avg) >= window_size else 0
                }
        
        return reward_learning
    
    def _analyze_frequency_preference(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æé¢‘ç‡åå¥½å­¦ä¹ """
        freq_pref = {}
        
        # é¢‘ç‡é€‰æ‹©ç»Ÿè®¡
        freq_counts = df['selected_frequency'].value_counts()
        freq_rewards = df.groupby('selected_frequency')['current_reward'].agg(['count', 'mean', 'std']).fillna(0)
        
        freq_pref['selection_statistics'] = {
            'frequency_counts': freq_counts.to_dict(),
            'frequency_rewards': freq_rewards.to_dict('index')
        }
        
        # æœ€ä¼˜é¢‘ç‡è¯†åˆ«
        best_freq_by_reward = freq_rewards['mean'].idxmax() if not freq_rewards.empty else None
        most_selected_freq = freq_counts.index[0] if not freq_counts.empty else None
        
        freq_pref['optimal_frequency_analysis'] = {
            'best_frequency_by_reward': best_freq_by_reward,
            'best_average_reward': freq_rewards.loc[best_freq_by_reward, 'mean'] if best_freq_by_reward else None,
            'most_selected_frequency': most_selected_freq,
            'most_selected_count': freq_counts.iloc[0] if not freq_counts.empty else None,
            'optimal_vs_preferred_match': best_freq_by_reward == most_selected_freq
        }
        
        # EDP (èƒ½æ•ˆ) åˆ†æ
        if 'edp_value' in df.columns:
            freq_edp = df.groupby('selected_frequency')['edp_value'].agg(['count', 'mean', 'std']).fillna(0)
            best_freq_by_edp = freq_edp['mean'].idxmin() if not freq_edp.empty else None  # EDPè¶Šå°è¶Šå¥½
            
            freq_pref['energy_efficiency_analysis'] = {
                'best_frequency_by_edp': best_freq_by_edp,
                'best_edp_value': freq_edp.loc[best_freq_by_edp, 'mean'] if best_freq_by_edp else None,
                'frequency_edp_stats': freq_edp.to_dict('index')
            }
        
        return freq_pref
    
    def _analyze_feature_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æç‰¹å¾æ¨¡å¼å’Œé‡è¦æ€§"""
        feature_analysis = {}
        
        # 7ç»´ç‰¹å¾åˆ†æ
        feature_cols = ['has_queue', 'prefill_throughput', 'decode_throughput', 
                       'packing_efficiency', 'concurrency', 'gpu_cache_usage', 'cache_hit_rate']
        
        available_features = [col for col in feature_cols if col in df.columns]
        
        if available_features:
            feature_stats = df[available_features].describe()
            feature_analysis['feature_statistics'] = feature_stats.to_dict()
            
            # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
            feature_correlation = df[available_features + ['current_reward']].corr()['current_reward'].drop('current_reward')
            feature_analysis['feature_reward_correlation'] = feature_correlation.to_dict()
            
            # ç‰¹å¾å˜åŒ–èŒƒå›´åˆ†æ
            feature_ranges = {}
            for feature in available_features:
                feature_data = df[feature].dropna()
                if not feature_data.empty:
                    feature_ranges[feature] = {
                        'min': feature_data.min(),
                        'max': feature_data.max(),
                        'range': feature_data.max() - feature_data.min(),
                        'std': feature_data.std()
                    }
            
            feature_analysis['feature_ranges'] = feature_ranges
        
        return feature_analysis
    
    def _analyze_performance_optimization(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½ä¼˜åŒ–æ•ˆæœ"""
        perf_opt = {}
        
        # TTFT (Time To First Token) ä¼˜åŒ–
        if 'avg_ttft' in df.columns:
            ttft_data = df['avg_ttft'].dropna()
            if not ttft_data.empty:
                perf_opt['ttft_optimization'] = {
                    'initial_ttft': ttft_data.iloc[0],
                    'final_ttft': ttft_data.iloc[-1],
                    'ttft_improvement': ttft_data.iloc[0] - ttft_data.iloc[-1],  # å‡å°‘æ˜¯æ”¹å–„
                    'ttft_improvement_ratio': (ttft_data.iloc[0] - ttft_data.iloc[-1]) / ttft_data.iloc[0] * 100 if ttft_data.iloc[0] != 0 else 0,
                    'average_ttft': ttft_data.mean(),
                    'ttft_volatility': ttft_data.std()
                }
        
        # TPOT (Time Per Output Token) ä¼˜åŒ–
        if 'avg_tpot' in df.columns:
            tpot_data = df['avg_tpot'].dropna()
            if not tpot_data.empty:
                perf_opt['tpot_optimization'] = {
                    'initial_tpot': tpot_data.iloc[0],
                    'final_tpot': tpot_data.iloc[-1],
                    'tpot_improvement': tpot_data.iloc[0] - tpot_data.iloc[-1],  # å‡å°‘æ˜¯æ”¹å–„
                    'tpot_improvement_ratio': (tpot_data.iloc[0] - tpot_data.iloc[-1]) / tpot_data.iloc[0] * 100 if tpot_data.iloc[0] != 0 else 0,
                    'average_tpot': tpot_data.mean(),
                    'tpot_volatility': tpot_data.std()
                }
        
        # æ€»å»¶è¿Ÿä¼˜åŒ–
        if 'total_latency' in df.columns:
            latency_data = df['total_latency'].dropna()
            if not latency_data.empty:
                perf_opt['latency_optimization'] = {
                    'initial_latency': latency_data.iloc[0],
                    'final_latency': latency_data.iloc[-1],
                    'latency_improvement': latency_data.iloc[0] - latency_data.iloc[-1],
                    'latency_improvement_ratio': (latency_data.iloc[0] - latency_data.iloc[-1]) / latency_data.iloc[0] * 100 if latency_data.iloc[0] != 0 else 0
                }
        
        return perf_opt
    
    def _analyze_energy_efficiency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æèƒ½æ•ˆä¼˜åŒ–"""
        energy_eff = {}
        
        # EDP (Energy-Delay Product) åˆ†æ
        if 'edp_value' in df.columns:
            edp_data = df['edp_value'].dropna()
            if not edp_data.empty:
                energy_eff['edp_optimization'] = {
                    'initial_edp': edp_data.iloc[0],
                    'final_edp': edp_data.iloc[-1],
                    'edp_improvement': edp_data.iloc[0] - edp_data.iloc[-1],  # EDPå‡å°‘æ˜¯æ”¹å–„
                    'edp_improvement_ratio': (edp_data.iloc[0] - edp_data.iloc[-1]) / edp_data.iloc[0] * 100 if edp_data.iloc[0] != 0 else 0,
                    'average_edp': edp_data.mean(),
                    'best_edp': edp_data.min(),
                    'worst_edp': edp_data.max(),
                    'edp_volatility': edp_data.std()
                }
        
        # èƒ½è€—åˆ†æ
        if 'energy_delta_j' in df.columns:
            energy_data = df['energy_delta_j'].dropna()
            if not energy_data.empty:
                energy_eff['energy_consumption'] = {
                    'average_energy_per_round': energy_data.mean(),
                    'total_energy_consumed': energy_data.sum(),
                    'energy_efficiency_trend': self._calculate_trend(energy_data),
                    'min_energy': energy_data.min(),
                    'max_energy': energy_data.max()
                }
        
        # èƒ½æ•ˆæŒ‡æ ‡
        if 'energy_efficiency' in df.columns:
            eff_data = df['energy_efficiency'].dropna()
            if not eff_data.empty:
                energy_eff['efficiency_metrics'] = {
                    'initial_efficiency': eff_data.iloc[0],
                    'final_efficiency': eff_data.iloc[-1],
                    'efficiency_improvement': eff_data.iloc[-1] - eff_data.iloc[0],
                    'efficiency_improvement_ratio': (eff_data.iloc[-1] - eff_data.iloc[0]) / eff_data.iloc[0] * 100 if eff_data.iloc[0] != 0 else 0,
                    'average_efficiency': eff_data.mean()
                }
        
        return energy_eff
    
    def _analyze_adaptability(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†æè‡ªé€‚åº”èƒ½åŠ›"""
        adaptability = {}
        
        # å·¥ä½œè´Ÿè½½å˜åŒ–é€‚åº”æ€§
        feature_cols = ['has_queue', 'prefill_throughput', 'decode_throughput', 
                       'packing_efficiency', 'concurrency', 'gpu_cache_usage', 'cache_hit_rate']
        
        available_features = [col for col in feature_cols if col in df.columns]
        
        if available_features:
            # è®¡ç®—ç‰¹å¾å˜åŒ–å¹…åº¦
            feature_changes = {}
            for feature in available_features:
                feature_data = df[feature].dropna()
                if len(feature_data) > 1:
                    # è®¡ç®—ç›¸é‚»å€¼ä¹‹é—´çš„å˜åŒ–
                    changes = abs(feature_data.diff()).dropna()
                    feature_changes[feature] = {
                        'average_change': changes.mean(),
                        'max_change': changes.max(),
                        'change_frequency': len(changes[changes > 0]) / len(changes) * 100 if len(changes) > 0 else 0
                    }
            
            adaptability['workload_adaptability'] = feature_changes
        
        # é¢‘ç‡è°ƒæ•´é€‚åº”æ€§
        freq_changes = df['selected_frequency'].diff().dropna()
        if not freq_changes.empty:
            adaptability['frequency_adaptability'] = {
                'total_frequency_changes': len(freq_changes[freq_changes != 0]),
                'frequency_change_rate': len(freq_changes[freq_changes != 0]) / len(df) * 100 if len(df) > 0 else 0,
                'average_frequency_change': abs(freq_changes[freq_changes != 0]).mean() if len(freq_changes[freq_changes != 0]) > 0 else 0
            }
        
        return adaptability
    
    def _evaluate_learning_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """è¯„ä¼°åœ¨çº¿å­¦ä¹ è´¨é‡"""
        quality = {}
        
        # å­¦ä¹ æ•ˆç‡è¯„ä¼°
        learning_rounds = len(df[df['phase'] == 'LEARNING'])
        total_rounds = len(df)
        
        quality['learning_efficiency'] = {
            'learning_rounds': learning_rounds,
            'total_rounds': total_rounds,
            'learning_efficiency_ratio': learning_rounds / total_rounds * 100 if total_rounds > 0 else 0,
            'convergence_speed': 'Fast' if learning_rounds < total_rounds * 0.3 else ('Medium' if learning_rounds < total_rounds * 0.7 else 'Slow')
        }
        
        # å†³ç­–è´¨é‡è¯„ä¼°
        if 'current_reward' in df.columns:
            rewards = df['current_reward'].dropna()
            if not rewards.empty:
                positive_rewards = len(rewards[rewards > 0])
                quality['decision_quality'] = {
                    'positive_reward_ratio': positive_rewards / len(rewards) * 100,
                    'reward_consistency': 1.0 - (rewards.std() / max(abs(rewards.mean()), 1e-6)),
                    'learning_stability': 'Stable' if rewards.std() < 0.1 else 'Volatile'
                }
        
        # ç®—æ³•æ€§èƒ½è¯„ä¼°
        if 'edp_value' in df.columns:
            edp_data = df['edp_value'].dropna()
            if len(edp_data) > 1:
                edp_trend = self._calculate_trend(edp_data)  # è´Ÿè¶‹åŠ¿è¡¨ç¤ºEDPåœ¨å‡å°‘ï¼ˆæ”¹å–„ï¼‰
                quality['algorithm_performance'] = {
                    'edp_trend': edp_trend,
                    'optimization_effectiveness': 'Excellent' if edp_trend < -0.1 else ('Good' if edp_trend < 0 else 'Poor'),
                    'performance_improvement': 'Yes' if edp_trend < 0 else 'No'
                }
        
        return quality
    
    def _calculate_gini_coefficient(self, values):
        """è®¡ç®—åŸºå°¼ç³»æ•°"""
        if len(values) == 0:
            return 0
        sorted_values = np.sort(values)
        n = len(values)
        cumsum = np.cumsum(sorted_values)
        return (2 * np.sum((np.arange(1, n + 1) * sorted_values))) / (n * cumsum[-1]) - (n + 1) / n
    
    def _calculate_entropy(self, values):
        """è®¡ç®—ä¿¡æ¯ç†µ"""
        if len(values) == 0:
            return 0
        total = sum(values)
        probabilities = [v / total for v in values if v > 0]
        return -sum(p * np.log2(p) for p in probabilities)
    
    def _calculate_trend(self, data):
        """è®¡ç®—æ•°æ®è¶‹åŠ¿ (çº¿æ€§å›å½’æ–œç‡)"""
        if len(data) < 2:
            return 0
        x = np.arange(len(data))
        return np.polyfit(x, data, 1)[0]
    
    def create_comprehensive_visualizations(self, df: pd.DataFrame, analysis: Dict[str, Any]):
        """åˆ›å»ºå…¨é¢çš„å¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“ˆ ç”Ÿæˆå…¨é¢å¯è§†åŒ–å›¾è¡¨...")
        
        if df.empty:
            print("âš ï¸ æ²¡æœ‰æ•°æ®å¯è§†åŒ–")
            return
        
        # åˆ›å»ºä¸»è¦åˆ†æå›¾è¡¨
        fig = plt.figure(figsize=(24, 16))
        gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)
        
        # 1. å¥–åŠ±å­¦ä¹ æ›²çº¿
        ax1 = fig.add_subplot(gs[0, 0:2])
        self._plot_reward_learning_curve(df, ax1)
        
        # 2. é¢‘ç‡é€‰æ‹©æ¨¡å¼
        ax2 = fig.add_subplot(gs[0, 2:])
        self._plot_frequency_selection_pattern(df, ax2)
        
        # 3. EDPä¼˜åŒ–è¶‹åŠ¿
        ax3 = fig.add_subplot(gs[1, 0:2])
        self._plot_edp_optimization(df, ax3)
        
        # 4. æ¢ç´¢-åˆ©ç”¨é˜¶æ®µåˆ†å¸ƒ
        ax4 = fig.add_subplot(gs[1, 2])
        self._plot_exploration_exploitation_phases(df, ax4)
        
        # 5. ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾
        ax5 = fig.add_subplot(gs[1, 3])
        self._plot_feature_importance_heatmap(df, analysis, ax5)
        
        # 6. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
        ax6 = fig.add_subplot(gs[2, 0:2])
        self._plot_performance_metrics(df, ax6)
        
        # 7. é¢‘ç‡-å¥–åŠ±å…³ç³»
        ax7 = fig.add_subplot(gs[2, 2:])
        self._plot_frequency_reward_relationship(df, ax7)
        
        # 8. èƒ½æ•ˆåˆ†æ
        ax8 = fig.add_subplot(gs[3, 0:2])
        self._plot_energy_efficiency_analysis(df, ax8)
        
        # 9. å­¦ä¹ è´¨é‡è¯„ä¼°
        ax9 = fig.add_subplot(gs[3, 2:])
        self._plot_learning_quality_assessment(df, analysis, ax9)
        
        plt.suptitle(f'vLLM GPU Autoscaler - Online Learning Performance Analysis\nGPU: {analysis.get("basic_stats", {}).get("gpu_model", "Unknown")} | Algorithm: {analysis.get("basic_stats", {}).get("algorithm", "Unknown")}', 
                    fontsize=16, fontweight='bold')
        
        plt.savefig(f"{self.output_dir}/comprehensive_analysis.png", dpi=300, bbox_inches='tight')
        print(f"   ä¿å­˜: {self.output_dir}/comprehensive_analysis.png")
        
        # åˆ›å»ºæ”¶æ•›åˆ†æä¸“é—¨å›¾è¡¨
        self._create_convergence_analysis_plot(df, analysis)
        
        plt.show()
    
    def _plot_reward_learning_curve(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶å¥–åŠ±å­¦ä¹ æ›²çº¿"""
        if 'current_reward' not in df.columns:
            ax.text(0.5, 0.5, 'No Reward Data Available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Reward Learning Curve')
            return
        
        rounds = df['round']
        rewards = df['current_reward']
        avg_rewards = df['average_reward']
        
        # ç»˜åˆ¶åŸå§‹å¥–åŠ±
        ax.plot(rounds, rewards, alpha=0.4, color='lightblue', linewidth=1, label='Current Reward')
        
        # ç»˜åˆ¶å¹³å‡å¥–åŠ±
        ax.plot(rounds, avg_rewards, color='red', linewidth=2, label='Average Reward')
        
        # ç»˜åˆ¶ç§»åŠ¨å¹³å‡
        if len(rewards) > 10:
            window = min(20, len(rewards) // 5)
            moving_avg = rewards.rolling(window=window, center=True).mean()
            ax.plot(rounds, moving_avg, color='darkblue', linewidth=2, label=f'{window}-Round Moving Avg')
        
        # æ ‡è®°å­¦ä¹ å’Œåˆ©ç”¨é˜¶æ®µ
        learning_mask = df['phase'] == 'LEARNING'
        if learning_mask.any():
            ax.axvspan(rounds[learning_mask].min(), rounds[learning_mask].max(), 
                      alpha=0.2, color='orange', label='Learning Phase')
        
        exploitation_mask = df['phase'] == 'EXPLOITATION'
        if exploitation_mask.any():
            ax.axvspan(rounds[exploitation_mask].min(), rounds[exploitation_mask].max(), 
                      alpha=0.2, color='green', label='Exploitation Phase')
        
        ax.set_xlabel('Round')
        ax.set_ylabel('Reward')
        ax.set_title('Reward Learning Curve')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_frequency_selection_pattern(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶é¢‘ç‡é€‰æ‹©æ¨¡å¼"""
        freq_counts = df['selected_frequency'].value_counts().sort_index()
        
        # æ ¹æ®é˜¶æ®µè®¾ç½®é¢œè‰²
        phase_colors = []
        for freq in freq_counts.index:
            freq_data = df[df['selected_frequency'] == freq]
            if not freq_data.empty:
                phase = freq_data['phase'].iloc[0]
                phase_colors.append('lightcoral' if phase == 'LEARNING' else 'lightblue')
            else:
                phase_colors.append('lightcoral')  # é»˜è®¤å­¦ä¹ é˜¶æ®µé¢œè‰²
        
        bars = ax.bar(freq_counts.index, freq_counts.values, color=phase_colors, alpha=0.7)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, count in zip(bars, freq_counts.values):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                   str(count), ha='center', va='bottom', fontsize=8)
        
        ax.set_xlabel('Frequency (MHz)')
        ax.set_ylabel('Selection Count')
        ax.set_title('Frequency Selection Pattern')
        
        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='lightcoral', alpha=0.7, label='Learning Phase'),
                          Patch(facecolor='lightblue', alpha=0.7, label='Exploitation Phase')]
        ax.legend(handles=legend_elements)
        
        ax.grid(True, alpha=0.3)
    
    def _plot_edp_optimization(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶EDPä¼˜åŒ–è¶‹åŠ¿"""
        if 'edp_value' not in df.columns:
            ax.text(0.5, 0.5, 'No EDP Data Available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('EDP Optimization Trend')
            return
        
        rounds = df['round']
        edp_values = df['edp_value']
        
        # ç»˜åˆ¶EDPå€¼
        ax.plot(rounds, edp_values, color='purple', linewidth=1, alpha=0.6, label='EDP Value')
        
        # ç»˜åˆ¶ç§»åŠ¨å¹³å‡
        if len(edp_values) > 5:
            window = min(10, len(edp_values) // 3)
            moving_avg = edp_values.rolling(window=window, center=True).mean()
            ax.plot(rounds, moving_avg, color='darkred', linewidth=2, label=f'{window}-Round Moving Avg')
        
        # æ ‡è®°æœ€ä¼˜ç‚¹
        min_edp_idx = edp_values.idxmin()
        min_edp_round = rounds[min_edp_idx]
        min_edp_value = edp_values[min_edp_idx]
        ax.scatter(min_edp_round, min_edp_value, color='red', s=100, zorder=5, label=f'Best EDP: {min_edp_value:.2f}')
        
        # æ·»åŠ è¶‹åŠ¿çº¿
        if len(edp_values) > 2:
            z = np.polyfit(rounds, edp_values, 1)
            p = np.poly1d(z)
            ax.plot(rounds, p(rounds), "--", color='orange', alpha=0.8, label=f'Trend (slope: {z[0]:.4f})')
        
        ax.set_xlabel('Round')
        ax.set_ylabel('EDP Value')
        ax.set_title('EDP Optimization Trend')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def _plot_exploration_exploitation_phases(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶æ¢ç´¢-åˆ©ç”¨é˜¶æ®µåˆ†å¸ƒ"""
        phase_counts = df['phase'].value_counts()
        
        # åŠ¨æ€åˆ›å»ºæ ‡ç­¾å’Œé¢œè‰²
        color_map = {'LEARNING': 'lightcoral', 'EXPLOITATION': 'lightblue'}
        label_map = {'LEARNING': 'Learning', 'EXPLOITATION': 'Exploitation'}
        
        colors = [color_map.get(phase, 'gray') for phase in phase_counts.index]
        labels = [label_map.get(phase, phase) for phase in phase_counts.index]
        
        if len(phase_counts) > 0:
            wedges, texts, autotexts = ax.pie(phase_counts.values, labels=labels, autopct='%1.1f%%', 
                                             colors=colors, startangle=90)
        else:
            ax.text(0.5, 0.5, 'No Phase Data Available', ha='center', va='center', transform=ax.transAxes)
        
        ax.set_title('Exploration vs Exploitation')
    
    def _plot_feature_importance_heatmap(self, df: pd.DataFrame, analysis: Dict[str, Any], ax):
        """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§çƒ­åŠ›å›¾"""
        feature_corr = analysis.get('feature_analysis', {}).get('feature_reward_correlation', {})
        
        if not feature_corr:
            ax.text(0.5, 0.5, 'No Feature Correlation Data', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Feature Importance')
            return
        
        features = list(feature_corr.keys())
        correlations = list(feature_corr.values())
        
        # åˆ›å»ºçƒ­åŠ›å›¾æ•°æ®
        corr_matrix = np.array(correlations).reshape(-1, 1)
        
        im = ax.imshow(corr_matrix, cmap='RdBu_r', aspect='auto', vmin=-1, vmax=1)
        
        # è®¾ç½®åˆ»åº¦
        ax.set_yticks(range(len(features)))
        ax.set_yticklabels([f.replace('_', ' ').title() for f in features])
        ax.set_xticks([0])
        ax.set_xticklabels(['Reward Correlation'])
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, corr in enumerate(correlations):
            ax.text(0, i, f'{corr:.3f}', ha='center', va='center', 
                   color='white' if abs(corr) > 0.5 else 'black', fontweight='bold')
        
        ax.set_title('Feature-Reward Correlation')
        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
    
    def _plot_performance_metrics(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”"""
        if 'avg_ttft' not in df.columns or 'avg_tpot' not in df.columns:
            ax.text(0.5, 0.5, 'No Performance Data Available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Performance Metrics')
            return
        
        rounds = df['round']
        ttft = df['avg_ttft']
        tpot = df['avg_tpot']
        
        ax2 = ax.twinx()
        
        line1 = ax.plot(rounds, ttft, color='blue', linewidth=2, label='TTFT (s)')
        line2 = ax2.plot(rounds, tpot, color='red', linewidth=2, label='TPOT (s)')
        
        ax.set_xlabel('Round')
        ax.set_ylabel('TTFT (seconds)', color='blue')
        ax2.set_ylabel('TPOT (seconds)', color='red')
        ax.set_title('Performance Metrics Trend')
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax.legend(lines, labels, loc='upper right')
        
        ax.grid(True, alpha=0.3)
    
    def _plot_frequency_reward_relationship(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶é¢‘ç‡-å¥–åŠ±å…³ç³»"""
        freq_reward_stats = df.groupby('selected_frequency')['current_reward'].agg(['mean', 'std', 'count']).reset_index()
        
        # æ°”æ³¡å›¾ï¼šx=é¢‘ç‡, y=å¹³å‡å¥–åŠ±, å¤§å°=é€‰æ‹©æ¬¡æ•°
        scatter = ax.scatter(freq_reward_stats['selected_frequency'], freq_reward_stats['mean'], 
                           s=freq_reward_stats['count']*20, alpha=0.6, c=freq_reward_stats['mean'], 
                           cmap='RdYlBu_r')
        
        # æ·»åŠ è¯¯å·®æ¡
        ax.errorbar(freq_reward_stats['selected_frequency'], freq_reward_stats['mean'], 
                   yerr=freq_reward_stats['std'], fmt='none', color='black', alpha=0.5)
        
        ax.set_xlabel('Frequency (MHz)')
        ax.set_ylabel('Average Reward')
        ax.set_title('Frequency-Reward Relationship')
        
        # æ·»åŠ æœ€ä½³é¢‘ç‡æ ‡è®°
        best_freq_idx = freq_reward_stats['mean'].idxmax()
        best_freq = freq_reward_stats.loc[best_freq_idx, 'selected_frequency']
        best_reward = freq_reward_stats.loc[best_freq_idx, 'mean']
        ax.annotate(f'Best: {best_freq}MHz\n({best_reward:.3f})', 
                   xy=(best_freq, best_reward), xytext=(10, 10), 
                   textcoords='offset points', bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),
                   arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))
        
        plt.colorbar(scatter, ax=ax, label='Average Reward')
        ax.grid(True, alpha=0.3)
    
    def _plot_energy_efficiency_analysis(self, df: pd.DataFrame, ax):
        """ç»˜åˆ¶èƒ½æ•ˆåˆ†æ"""
        if 'energy_delta_j' not in df.columns:
            ax.text(0.5, 0.5, 'No Energy Data Available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Energy Efficiency Analysis')
            return
        
        rounds = df['round']
        energy = df['energy_delta_j']
        efficiency = df['energy_efficiency']
        
        ax2 = ax.twinx()
        
        line1 = ax.plot(rounds, energy, color='orange', linewidth=2, alpha=0.7, label='Energy Consumption (J)')
        line2 = ax2.plot(rounds, efficiency, color='green', linewidth=2, label='Energy Efficiency (1/EDP)')
        
        ax.set_xlabel('Round')
        ax.set_ylabel('Energy Consumption (J)', color='orange')
        ax2.set_ylabel('Energy Efficiency', color='green')
        ax.set_title('Energy Efficiency Analysis')
        
        # åˆå¹¶å›¾ä¾‹
        lines = line1 + line2
        labels = [l.get_label() for l in lines]
        ax.legend(lines, labels, loc='upper right')
        
        ax.grid(True, alpha=0.3)
    
    def _plot_learning_quality_assessment(self, df: pd.DataFrame, analysis: Dict[str, Any], ax):
        """ç»˜åˆ¶å­¦ä¹ è´¨é‡è¯„ä¼°"""
        quality_metrics = {}
        
        # æ”¶é›†è´¨é‡æŒ‡æ ‡
        convergence = analysis.get('convergence_analysis', {})
        learning_quality = analysis.get('learning_quality', {})
        
        if convergence.get('convergence_achieved'):
            quality_metrics['Convergence'] = 100
        else:
            quality_metrics['Convergence'] = 0
        
        learning_eff = learning_quality.get('learning_efficiency', {})
        if 'learning_efficiency_ratio' in learning_eff:
            quality_metrics['Learning Efficiency'] = 100 - learning_eff['learning_efficiency_ratio']  # è¶Šå°‘è¶Šå¥½
        
        decision_qual = learning_quality.get('decision_quality', {})
        if 'positive_reward_ratio' in decision_qual:
            quality_metrics['Decision Quality'] = decision_qual['positive_reward_ratio']
        
        algo_perf = learning_quality.get('algorithm_performance', {})
        if 'edp_trend' in algo_perf:
            # å°†EDPè¶‹åŠ¿è½¬æ¢ä¸ºæ€§èƒ½åˆ†æ•°
            edp_trend = algo_perf['edp_trend']
            if edp_trend < -0.1:
                quality_metrics['Performance'] = 100
            elif edp_trend < 0:
                quality_metrics['Performance'] = 70
            else:
                quality_metrics['Performance'] = 30
        
        if not quality_metrics:
            ax.text(0.5, 0.5, 'No Quality Data Available', ha='center', va='center', transform=ax.transAxes)
            ax.set_title('Learning Quality Assessment')
            return
        
        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2 * np.pi, len(quality_metrics), endpoint=False)
        values = list(quality_metrics.values())
        
        # é—­åˆå›¾å½¢
        angles = np.concatenate((angles, [angles[0]]))
        values = np.concatenate((values, [values[0]]))
        
        ax.plot(angles, values, 'o-', linewidth=2, color='blue')
        ax.fill(angles, values, alpha=0.25, color='blue')
        
        # è®¾ç½®æ ‡ç­¾
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(quality_metrics.keys())
        ax.set_ylim(0, 100)
        ax.set_title('Learning Quality Assessment\n(Score: 0-100)')
        ax.grid(True)
        
        # æ·»åŠ åˆ†æ•°æ ‡ç­¾
        for angle, value, label in zip(angles[:-1], values[:-1], quality_metrics.keys()):
            ax.annotate(f'{value:.1f}', xy=(angle, value), xytext=(5, 5), 
                       textcoords='offset points', fontsize=8)
    
    def _create_convergence_analysis_plot(self, df: pd.DataFrame, analysis: Dict[str, Any]):
        """åˆ›å»ºæ”¶æ•›åˆ†æä¸“é—¨å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Convergence Analysis', fontsize=16, fontweight='bold')
        
        # 1. åŠ¨ä½œé€‰æ‹©ç¨³å®šæ€§
        ax = axes[0, 0]
        if not df.empty:
            window_size = 20
            rolling_mode = df['selected_frequency'].rolling(window=window_size).apply(lambda x: x.mode()[0] if not x.empty else 0)
            stability = (df['selected_frequency'] == rolling_mode).rolling(window=window_size).mean() * 100
            
            ax.plot(df['round'], stability, color='blue', linewidth=2)
            ax.axhline(y=60, color='red', linestyle='--', alpha=0.7, label='Convergence Threshold (60%)')
            ax.set_xlabel('Round')
            ax.set_ylabel('Action Stability (%)')
            ax.set_title('Action Selection Stability')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        # 2. å¥–åŠ±æ–¹å·®åˆ†æ
        ax = axes[0, 1]
        if 'current_reward' in df.columns:
            window_size = 15
            rolling_var = df['current_reward'].rolling(window=window_size).var()
            
            ax.plot(df['round'], rolling_var, color='green', linewidth=2)
            ax.set_xlabel('Round')
            ax.set_ylabel('Reward Variance')
            ax.set_title('Reward Variance Trend')
            ax.grid(True, alpha=0.3)
        
        # 3. æ¢ç´¢é¢‘ç‡è¡°å‡
        ax = axes[1, 0]
        exploration_ratio = []
        window = 20
        for i in range(len(df)):
            start_idx = max(0, i - window + 1)
            recent_frequencies = df['selected_frequency'].iloc[start_idx:i+1]
            unique_count = recent_frequencies.nunique()
            total_available = df['n_available_frequencies'].iloc[i]
            exploration_ratio.append(unique_count / total_available * 100 if total_available > 0 else 0)
        
        ax.plot(df['round'], exploration_ratio, color='orange', linewidth=2)
        ax.set_xlabel('Round')
        ax.set_ylabel('Exploration Ratio (%)')
        ax.set_title('Exploration Frequency Decay')
        ax.grid(True, alpha=0.3)
        
        # 4. æ”¶æ•›çŠ¶æ€æ€»ç»“
        ax = axes[1, 1]
        convergence = analysis.get('convergence_analysis', {})
        
        # åˆ›å»ºæ”¶æ•›æŒ‡æ ‡æ€»ç»“è¡¨
        convergence_data = []
        if convergence.get('convergence_achieved'):
            convergence_data.append(['Convergence Status', 'âœ… Achieved'])
            convergence_data.append(['Rounds to Convergence', f"{convergence.get('rounds_to_convergence', 'N/A')}"])
            convergence_data.append(['Dominant Frequency', f"{convergence.get('dominant_frequency', 'N/A')} MHz"])
            convergence_data.append(['Frequency Stability', f"{convergence.get('frequency_stability', 0):.1f}%"])
        else:
            convergence_data.append(['Convergence Status', 'âŒ Not Achieved'])
            convergence_data.append(['Learning Progress', f"{analysis.get('basic_stats', {}).get('learning_rounds', 0)} rounds"])
        
        # æ˜¾ç¤ºè¡¨æ ¼
        table = ax.table(cellText=convergence_data, cellLoc='left', loc='center', colWidths=[0.5, 0.5])
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)
        
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        ax.axis('off')
        ax.set_title('Convergence Summary')
        
        plt.tight_layout()
        plt.savefig(f"{self.output_dir}/convergence_analysis.png", dpi=300, bbox_inches='tight')
        print(f"   ä¿å­˜: {self.output_dir}/convergence_analysis.png")
    
    def generate_analysis_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report = []
        report.append("# vLLM GPU Autoscaler åœ¨çº¿å­¦ä¹ æ•ˆæœåˆ†ææŠ¥å‘Š")
        report.append("=" * 50)
        report.append("")
        
        # åŸºç¡€ä¿¡æ¯
        basic_stats = analysis.get('basic_stats', {})
        report.append("## åŸºç¡€ç»Ÿè®¡ä¿¡æ¯")
        report.append(f"- æ€»è½®æ¬¡: {basic_stats.get('total_rounds', 0)}")
        report.append(f"- å­¦ä¹ è½®æ¬¡: {basic_stats.get('learning_rounds', 0)}")
        report.append(f"- åˆ©ç”¨è½®æ¬¡: {basic_stats.get('exploitation_rounds', 0)}")
        report.append(f"- å°è¯•é¢‘ç‡æ•°: {basic_stats.get('unique_frequencies_tried', 0)}")
        report.append(f"- å¯ç”¨é¢‘ç‡æ•°: {basic_stats.get('total_frequencies_available', 0)}")
        report.append(f"- ç®—æ³•: {basic_stats.get('algorithm', 'Unknown')}")
        report.append(f"- GPUå‹å·: {basic_stats.get('gpu_model', 'Unknown')}")
        report.append("")
        
        # æ”¶æ•›åˆ†æ
        convergence = analysis.get('convergence_analysis', {})
        report.append("## æ”¶æ•›æ€§åˆ†æ")
        if convergence.get('convergence_achieved'):
            report.append("âœ… **æ¨¡å‹å·²æ”¶æ•›**")
            report.append(f"- æ”¶æ•›è½®æ¬¡: {convergence.get('rounds_to_convergence', 'N/A')}")
            report.append(f"- æ”¶æ•›æ¯”ä¾‹: {convergence.get('convergence_ratio', 0):.1f}%")
            report.append(f"- ä¸»å¯¼é¢‘ç‡: {convergence.get('dominant_frequency', 'N/A')} MHz")
            report.append(f"- é¢‘ç‡ç¨³å®šæ€§: {convergence.get('frequency_stability', 0):.1f}%")
        else:
            report.append("âŒ **æ¨¡å‹æœªæ”¶æ•›**")
            report.append("- å»ºè®®å¢åŠ è®­ç»ƒè½®æ¬¡æˆ–è°ƒæ•´ç®—æ³•å‚æ•°")
        report.append("")
        
        # æ¨¡å¼åˆ‡æ¢äº‹ä»¶åˆ†æ
        mode_switching = analysis.get('mode_switching', {})
        report.append("## æ¨¡å¼åˆ‡æ¢äº‹ä»¶åˆ†æ")
        
        if mode_switching.get('has_mode_switches'):
            report.append(f"- åˆ‡æ¢äº‹ä»¶æ€»æ•°: {mode_switching.get('events_count', 0)}")
            report.append(f"- æ”¶æ•›äº‹ä»¶: {mode_switching.get('convergence_events', 0)}")
            report.append(f"- æ€§èƒ½é€€åŒ–äº‹ä»¶: {mode_switching.get('degradation_events', 0)}")
            report.append(f"- å­¦ä¹ -åˆ©ç”¨å¾ªç¯: {mode_switching.get('learning_exploitation_cycles', 0)}")
            
            if 'avg_switch_interval_seconds' in mode_switching:
                report.append(f"- å¹³å‡åˆ‡æ¢é—´éš”: {mode_switching['avg_switch_interval_seconds']:.1f}ç§’")
            
            report.append(f"- æ”¶æ•›è¡Œä¸º: {mode_switching.get('convergence_message', 'N/A')}")
            report.append(f"- é€‚åº”è¡Œä¸º: {mode_switching.get('adaptation_message', 'N/A')}")
        else:
            report.append("- çŠ¶æ€: æœªæ£€æµ‹åˆ°æ¨¡å¼åˆ‡æ¢äº‹ä»¶")
        report.append("")
        
        # å­¦ä¹ è´¨é‡è¯„ä¼°
        learning_quality = analysis.get('learning_quality', {})
        report.append("## å­¦ä¹ è´¨é‡è¯„ä¼°")
        
        learning_eff = learning_quality.get('learning_efficiency', {})
        if learning_eff:
            report.append(f"- å­¦ä¹ æ•ˆç‡: {learning_eff.get('convergence_speed', 'Unknown')}")
            report.append(f"- å­¦ä¹ æ•ˆç‡æ¯”ä¾‹: {learning_eff.get('learning_efficiency_ratio', 0):.1f}%")
        
        decision_qual = learning_quality.get('decision_quality', {})
        if decision_qual:
            report.append(f"- å†³ç­–è´¨é‡: æ­£å¥–åŠ±æ¯”ä¾‹ {decision_qual.get('positive_reward_ratio', 0):.1f}%")
            report.append(f"- å­¦ä¹ ç¨³å®šæ€§: {decision_qual.get('learning_stability', 'Unknown')}")
        
        algo_perf = learning_quality.get('algorithm_performance', {})
        if algo_perf:
            report.append(f"- ç®—æ³•æ€§èƒ½: {algo_perf.get('optimization_effectiveness', 'Unknown')}")
            report.append(f"- æ€§èƒ½æ”¹å–„: {algo_perf.get('performance_improvement', 'Unknown')}")
        report.append("")
        
        # æ¢ç´¢-åˆ©ç”¨åˆ†æ
        exp_exp = analysis.get('exploration_exploitation', {})
        report.append("## æ¢ç´¢-åˆ©ç”¨æƒè¡¡åˆ†æ")
        
        freq_diversity = exp_exp.get('frequency_diversity', {})
        if freq_diversity:
            report.append(f"- é¢‘ç‡å¤šæ ·æ€§: {freq_diversity.get('unique_frequencies', 0)} ä¸ªä¸åŒé¢‘ç‡")
            report.append(f"- é€‰æ‹©ç†µ: {freq_diversity.get('entropy', 0):.3f}")
            report.append(f"- æœ€å¸¸é€‰æ‹©é¢‘ç‡: {freq_diversity.get('most_selected_frequency', 'N/A')} MHz ({freq_diversity.get('most_selected_ratio', 0):.1f}%)")
        
        exploration_pattern = exp_exp.get('exploration_pattern', {})
        if exploration_pattern:
            report.append(f"- æ¢ç´¢é˜¶æ®µé¢‘ç‡æ•°: {exploration_pattern.get('frequencies_explored', 0)}")
            report.append(f"- æ¢ç´¢å‡åŒ€æ€§: {exploration_pattern.get('exploration_uniformity', 0):.3f}")
        
        exploitation_pattern = exp_exp.get('exploitation_pattern', {})
        if exploitation_pattern:
            report.append(f"- åˆ©ç”¨é˜¶æ®µé›†ä¸­åº¦: {exploitation_pattern.get('exploitation_concentration', 0):.1f}%")
        report.append("")
        
        # æ€§èƒ½ä¼˜åŒ–æ•ˆæœ
        perf_opt = analysis.get('performance_optimization', {})
        report.append("## æ€§èƒ½ä¼˜åŒ–æ•ˆæœ")
        
        ttft_opt = perf_opt.get('ttft_optimization', {})
        if ttft_opt:
            report.append(f"- TTFTä¼˜åŒ–: {ttft_opt.get('ttft_improvement', 0):.4f}s ({ttft_opt.get('ttft_improvement_ratio', 0):.1f}%)")
        
        tpot_opt = perf_opt.get('tpot_optimization', {})
        if tpot_opt:
            report.append(f"- TPOTä¼˜åŒ–: {tpot_opt.get('tpot_improvement', 0):.4f}s ({tpot_opt.get('tpot_improvement_ratio', 0):.1f}%)")
        
        latency_opt = perf_opt.get('latency_optimization', {})
        if latency_opt:
            report.append(f"- æ€»å»¶è¿Ÿä¼˜åŒ–: {latency_opt.get('latency_improvement', 0):.4f}s ({latency_opt.get('latency_improvement_ratio', 0):.1f}%)")
        report.append("")
        
        # èƒ½æ•ˆåˆ†æ
        energy_eff = analysis.get('energy_efficiency', {})
        report.append("## èƒ½æ•ˆä¼˜åŒ–åˆ†æ")
        
        edp_opt = energy_eff.get('edp_optimization', {})
        if edp_opt:
            report.append(f"- EDPä¼˜åŒ–: {edp_opt.get('edp_improvement', 0):.3f} ({edp_opt.get('edp_improvement_ratio', 0):.1f}%)")
            report.append(f"- æœ€ä½³EDP: {edp_opt.get('best_edp', 0):.3f}")
            report.append(f"- å¹³å‡EDP: {edp_opt.get('average_edp', 0):.3f}")
        
        efficiency_metrics = energy_eff.get('efficiency_metrics', {})
        if efficiency_metrics:
            report.append(f"- èƒ½æ•ˆæå‡: {efficiency_metrics.get('efficiency_improvement_ratio', 0):.1f}%")
        report.append("")
        
        # é¢‘ç‡åå¥½åˆ†æ
        freq_pref = analysis.get('frequency_preference', {})
        report.append("## é¢‘ç‡åå¥½å­¦ä¹ ")
        
        optimal_freq = freq_pref.get('optimal_frequency_analysis', {})
        if optimal_freq:
            report.append(f"- æœ€ä¼˜é¢‘ç‡(å¥–åŠ±): {optimal_freq.get('best_frequency_by_reward', 'N/A')} MHz")
            report.append(f"- æœ€å¸¸é€‰æ‹©é¢‘ç‡: {optimal_freq.get('most_selected_frequency', 'N/A')} MHz")
            report.append(f"- åå¥½åŒ¹é…: {'âœ…' if optimal_freq.get('optimal_vs_preferred_match') else 'âŒ'}")
        
        energy_analysis = freq_pref.get('energy_efficiency_analysis', {})
        if energy_analysis:
            report.append(f"- æœ€ä¼˜é¢‘ç‡(EDP): {energy_analysis.get('best_frequency_by_edp', 'N/A')} MHz")
            report.append(f"- æœ€ä½³EDPå€¼: {energy_analysis.get('best_edp_value', 0):.3f}")
        report.append("")
        
        # æ€»ç»“å»ºè®®
        report.append("## æ€»ç»“ä¸å»ºè®®")
        
        if convergence.get('convergence_achieved'):
            report.append("âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œå·²è¾¾åˆ°æ”¶æ•›çŠ¶æ€")
        else:
            report.append("âš ï¸ æ¨¡å‹å°šæœªæ”¶æ•›ï¼Œå»ºè®®ï¼š")
            report.append("  - å¢åŠ è®­ç»ƒè½®æ¬¡")
            report.append("  - è°ƒæ•´æ¢ç´¢å‚æ•°")
            report.append("  - æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")
        
        if algo_perf.get('optimization_effectiveness') == 'Excellent':
            report.append("âœ… ç®—æ³•æ€§èƒ½ä¼˜ç§€ï¼ŒEDPæŒç»­æ”¹å–„")
        elif algo_perf.get('optimization_effectiveness') == 'Good':
            report.append("ğŸ‘ ç®—æ³•æ€§èƒ½è‰¯å¥½ï¼Œæœ‰ä¸€å®šæ”¹å–„æ•ˆæœ")
        else:
            report.append("âš ï¸ ç®—æ³•æ€§èƒ½æœ‰å¾…æå‡ï¼Œå»ºè®®ä¼˜åŒ–å‚æ•°")
        
        if optimal_freq.get('optimal_vs_preferred_match'):
            report.append("âœ… å­¦ä¹ æ•ˆæœè‰¯å¥½ï¼Œæœ€ä¼˜é¢‘ç‡ä¸åå¥½é¢‘ç‡ä¸€è‡´")
        else:
            report.append("âš ï¸ å¯èƒ½å­˜åœ¨æ¢ç´¢ä¸å……åˆ†ï¼Œå»ºè®®å¢åŠ æ¢ç´¢")
        
        report_text = "\n".join(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = f"{self.output_dir}/analysis_report.txt"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        print(f"   æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
        return report_text
    
    def save_analysis_json(self, analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœä¸ºJSON"""
        json_path = f"{self.output_dir}/analysis_report.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        print(f"   JSONåˆ†æç»“æœå·²ä¿å­˜: {json_path}")
    
    def run_comprehensive_analysis(self, log_file_path: str = None, gpu_model: str = None):
        """è¿è¡Œå…¨é¢åˆ†æ"""
        print("ğŸš€ å¼€å§‹å…¨é¢åœ¨çº¿å­¦ä¹ æ•ˆæœåˆ†æ...")
        
        # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶
        if not log_file_path:
            log_file_path = self.find_latest_log(gpu_model)
            if not log_file_path:
                print("âŒ æœªæ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
                return
        
        # è§£ææ—¥å¿—
        df = self.parse_log(log_file_path)
        if df.empty:
            print("âŒ æ—¥å¿—è§£æå¤±è´¥æˆ–æ— æ•°æ®")
            return
        
        # è¿›è¡Œå…¨é¢åˆ†æ
        analysis = self.analyze_learning_effectiveness(df)
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.create_comprehensive_visualizations(df, analysis)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_analysis_report(analysis)
        
        # ä¿å­˜JSONç»“æœ
        self.save_analysis_json(analysis)
        
        print("=" * 60)
        print("ğŸ“Š **åˆ†æå®Œæˆï¼ä¸»è¦å‘ç°ï¼š**")
        print("=" * 60)
        
        basic_stats = analysis.get('basic_stats', {})
        convergence = analysis.get('convergence_analysis', {})
        learning_quality = analysis.get('learning_quality', {})
        
        print(f"ğŸ¯ **åŸºç¡€ç»Ÿè®¡**: {basic_stats.get('total_rounds', 0)} è½®, {basic_stats.get('unique_frequencies_tried', 0)} é¢‘ç‡")
        
        if convergence.get('convergence_achieved'):
            print(f"âœ… **æ”¶æ•›çŠ¶æ€**: å·²æ”¶æ•› ({convergence.get('rounds_to_convergence', 'N/A')} è½®)")
            print(f"ğŸ² **ä¸»å¯¼é¢‘ç‡**: {convergence.get('dominant_frequency', 'N/A')} MHz ({convergence.get('frequency_stability', 0):.1f}% ç¨³å®šæ€§)")
        else:
            print("âŒ **æ”¶æ•›çŠ¶æ€**: æœªæ”¶æ•›")
        
        algo_perf = learning_quality.get('algorithm_performance', {})
        if algo_perf:
            print(f"âš¡ **ç®—æ³•æ€§èƒ½**: {algo_perf.get('optimization_effectiveness', 'Unknown')}")
        
        # æ€§èƒ½æ”¹å–„æ€»ç»“
        perf_opt = analysis.get('performance_optimization', {})
        energy_eff = analysis.get('energy_efficiency', {})
        
        ttft_improvement = perf_opt.get('ttft_optimization', {}).get('ttft_improvement_ratio', 0)
        tpot_improvement = perf_opt.get('tpot_optimization', {}).get('tpot_improvement_ratio', 0)
        edp_improvement = energy_eff.get('edp_optimization', {}).get('edp_improvement_ratio', 0)
        
        print(f"ğŸ“ˆ **æ€§èƒ½æ”¹å–„**: TTFT {ttft_improvement:+.1f}%, TPOT {tpot_improvement:+.1f}%, EDP {edp_improvement:+.1f}%")
        
        print("=" * 60)
        print(f"ğŸ“ **è¾“å‡ºæ–‡ä»¶**:")
        print(f"   - ç»¼åˆåˆ†æå›¾: {self.output_dir}/comprehensive_analysis.png")
        print(f"   - æ”¶æ•›åˆ†æå›¾: {self.output_dir}/convergence_analysis.png") 
        print(f"   - åˆ†ææŠ¥å‘Š: {self.output_dir}/analysis_report.txt")
        print(f"   - JSONæ•°æ®: {self.output_dir}/analysis_report.json")
        print("=" * 60)
    
    def _analyze_mode_switching_events(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ¨¡å¼åˆ‡æ¢äº‹ä»¶"""
        print("ğŸ”„ åˆ†ææ¨¡å¼åˆ‡æ¢äº‹ä»¶...")
        
        events_analysis = {}
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å¼åˆ‡æ¢äº‹ä»¶è®°å½•
        if not hasattr(self, 'mode_switch_events') or not self.mode_switch_events:
            events_analysis['has_mode_switches'] = False
            events_analysis['events_count'] = 0
            events_analysis['convergence_events'] = 0
            events_analysis['degradation_events'] = 0
            events_analysis['message'] = 'æœªæ£€æµ‹åˆ°æ¨¡å¼åˆ‡æ¢äº‹ä»¶'
            return events_analysis
        
        events = self.mode_switch_events
        events_analysis['has_mode_switches'] = True
        events_analysis['events_count'] = len(events)
        
        # æŒ‰äº‹ä»¶ç±»å‹åˆ†ç±»ç»Ÿè®¡
        event_types = {}
        for event in events:
            event_type = event['event_type']
            if event_type not in event_types:
                event_types[event_type] = []
            event_types[event_type].append(event)
        
        events_analysis['event_types'] = event_types
        events_analysis['convergence_events'] = len(event_types.get('convergence_detected', []))
        events_analysis['degradation_events'] = len(event_types.get('performance_degradation', []))
        events_analysis['exploitation_switches'] = len(event_types.get('switch_to_exploitation', []))
        events_analysis['learning_switches'] = len(event_types.get('switch_to_learning', []))
        
        # åˆ†ææ¨¡å¼åˆ‡æ¢é¢‘ç‡
        if len(events) > 1:
            time_deltas = []
            for i in range(1, len(events)):
                delta = events[i]['timestamp'] - events[i-1]['timestamp']
                time_deltas.append(delta.total_seconds())
            
            if time_deltas:
                events_analysis['avg_switch_interval_seconds'] = np.mean(time_deltas)
                events_analysis['min_switch_interval_seconds'] = min(time_deltas)
                events_analysis['max_switch_interval_seconds'] = max(time_deltas)
        
        # æ£€æŸ¥å­¦ä¹ -åˆ©ç”¨å¾ªç¯æ¨¡å¼
        cycle_count = 0
        last_mode = None
        for event in events:
            if event['event_type'] in ['switch_to_exploitation', 'switch_to_learning']:
                current_mode = 'exploitation' if 'exploitation' in event['event_type'] else 'learning'
                if last_mode and last_mode != current_mode:
                    cycle_count += 1
                last_mode = current_mode
        
        events_analysis['learning_exploitation_cycles'] = cycle_count // 2  # å®Œæ•´å¾ªç¯æ•°
        
        # æ€»ç»“æ¨¡å¼åˆ‡æ¢æ•ˆæœ
        if events_analysis['convergence_events'] > 0:
            events_analysis['convergence_behavior'] = 'good'
            events_analysis['convergence_message'] = f'æ£€æµ‹åˆ° {events_analysis["convergence_events"]} æ¬¡æ”¶æ•›äº‹ä»¶'
        else:
            events_analysis['convergence_behavior'] = 'none'
            events_analysis['convergence_message'] = 'æœªæ£€æµ‹åˆ°æ”¶æ•›äº‹ä»¶'
        
        if events_analysis['degradation_events'] > 0:
            events_analysis['adaptation_behavior'] = 'reactive'
            events_analysis['adaptation_message'] = f'æ£€æµ‹åˆ° {events_analysis["degradation_events"]} æ¬¡æ€§èƒ½é€€åŒ–ï¼Œç³»ç»Ÿä¸»åŠ¨åˆ‡æ¢å›å­¦ä¹ æ¨¡å¼'
        else:
            events_analysis['adaptation_behavior'] = 'stable'
            events_analysis['adaptation_message'] = 'æ€§èƒ½ç¨³å®šï¼Œæ— é€€åŒ–æ£€æµ‹'
        
        return events_analysis


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    parser = argparse.ArgumentParser(description='vLLM GPU Autoscaler åœ¨çº¿å­¦ä¹ æ•ˆæœåˆ†æ')
    parser.add_argument('--log-file', type=str, help='æŒ‡å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--gpu-model', type=str, help='æŒ‡å®šGPUå‹å·ç›®å½• (å¦‚: A800_80GB_PCIe)')
    parser.add_argument('--output-dir', type=str, default='data/analysis', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œåˆ†æ
    analyzer = VLLMAutoscalerAnalyzer(output_dir=args.output_dir)
    analyzer.run_comprehensive_analysis(log_file_path=args.log_file, gpu_model=args.gpu_model)


if __name__ == "__main__":
    main()