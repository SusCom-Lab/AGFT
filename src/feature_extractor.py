import numpy as np
from typing import Dict, List, Optional
from .logger import setup_logger

logger = setup_logger(__name__)

class FeatureExtractor:
    """ç‰¹å¾æå–å™¨ - å¢å¼ºç‰ˆï¼ŒåŒ…å«GPUç¡¬ä»¶ç‰¹å¾"""
    
    def __init__(self, gpu_controller=None):
        """åˆå§‹åŒ–ç‰¹å¾æå–å™¨ - æ”¯æŒGPUç¡¬ä»¶ç‰¹å¾"""
        # å®šä¹‰åŸºç¡€ç‰¹å¾åç§°åˆ—è¡¨
        self.feature_names = [
            'has_queue', 
            'prefill_throughput', 
            'decode_throughput', 
            'packing_efficiency',
            'concurrency', 
            'gpu_cache_usage', 
            'cache_hit_rate'
            # æ³¨æ„ï¼šé¢‘ç‡ä¸ä½œä¸ºç¯å¢ƒç‰¹å¾ï¼Œè€Œæ˜¯åœ¨LinUCBä¸­ä½œä¸ºè¿ç»­åŠ¨ä½œç‰¹å¾å¤„ç†
        ]
        
        # è‡ªåŠ¨è®¡ç®—ç‰¹å¾æ•°é‡
        self.n_features = len(self.feature_names)
        
        # GPUæ§åˆ¶å™¨å¼•ç”¨ï¼ˆç”¨äºè·å–ç¡¬ä»¶ç‰¹å¾ï¼‰
        self.gpu_controller = gpu_controller
        
        # åœ¨çº¿ç‰¹å¾æ ‡å‡†åŒ–
        self.mean = np.zeros(self.n_features)
        self.M2 = np.zeros(self.n_features)
        self.count = 0
        
        logger.info(f"ğŸ”§ åˆå§‹åŒ–å¢å¼ºç‰¹å¾æå–å™¨: {self.n_features} ç»´ç‰¹å¾")
        logger.info(f"   ç¯å¢ƒç‰¹å¾: {', '.join(self.feature_names)} (7ç»´)")
        logger.info(f"   GPUæ§åˆ¶å™¨: {'å·²è¿æ¥' if gpu_controller else 'æœªè¿æ¥'}")
        
    def extract(self, gauge_metrics: Dict[str, float], 
                counter_deltas: Dict[str, float]) -> np.ndarray:
        """æå–ç‰¹å¾å‘é‡"""
        features = []
        
        # è·å–è¯·æ±‚æ•°é‡
        num_running = gauge_metrics.get('vllm:num_requests_running', 0)
        num_waiting = gauge_metrics.get('vllm:num_requests_waiting', 0)
        
        # 1. é˜Ÿåˆ—çŠ¶æ€ï¼ˆäºŒå€¼ï¼‰
        has_queue = 1.0 if num_waiting > 0 else 0.0
        features.append(has_queue)
        
        # 2. Prefillååï¼ˆtokens/2ç§’ï¼‰
        prefill_delta = counter_deltas.get('vllm:prompt_tokens_total_delta', 0)
        prefill_throughput = max(prefill_delta, 1e-3)
        features.append(prefill_throughput)
        
        # 3. Decodeååï¼ˆtokens/2ç§’ï¼‰
        decode_delta = counter_deltas.get('vllm:generation_tokens_total_delta', 0)
        decode_throughput = max(decode_delta, 1e-3)
        features.append(decode_throughput)

        
        # 4. Packingæ•ˆç‡
        delta_sum = counter_deltas.get('vllm:iteration_tokens_total_sum_delta', 0.0)
        delta_count = counter_deltas.get('vllm:iteration_tokens_total_count_delta', 0.0)
        packing_efficiency = delta_sum / delta_count if delta_count > 0 else 0.0
        features.append(packing_efficiency)
        
        # 5. å¹¶å‘åº¦
        concurrency = num_running
        features.append(concurrency)
        
        # 6. GPUç¼“å­˜ä½¿ç”¨ç‡
        gpu_cache = gauge_metrics.get('vllm:gpu_cache_usage_perc', 0) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        features.append(gpu_cache)
        
        # 7. ç¼“å­˜å‘½ä¸­ç‡
        hits = counter_deltas.get('vllm:gpu_prefix_cache_hits_total_delta', 0)
        queries = counter_deltas.get('vllm:gpu_prefix_cache_queries_total_delta', 1)
        cache_hit_rate = hits / max(queries, 1) *100
        features.append(cache_hit_rate)
        
        # ç¡®ä¿ç‰¹å¾æ•°é‡ä¸å®šä¹‰ä¸€è‡´
        assert len(features) == self.n_features, \
            f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…: {len(features)} vs {self.n_features}"
        
        # æ—¥å¿—ç‰¹å¾å€¼
        logger.debug("ğŸ“ˆ åŸå§‹ç‰¹å¾:")
        for name, value in zip(self.feature_names, features):
            logger.debug(f"  {name}: {value:.3f}")
            
        # è¿”å›numpyæ•°ç»„
        return np.asarray(features, dtype=np.float32)

    def normalize(self, features: np.ndarray) -> np.ndarray:
        """åœ¨çº¿ç‰¹å¾æ ‡å‡†åŒ–ï¼ˆWelfordç®—æ³•ï¼‰"""
        # ç±»å‹æ£€æŸ¥å’Œè½¬æ¢
        features = np.asarray(features, dtype=np.float32)
        
        # ç»´åº¦æ£€æŸ¥
        assert features.shape[0] == self.n_features, \
            f"ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: {features.shape[0]} vs {self.n_features}"
        
        # æ›´æ–°ç»Ÿè®¡é‡ï¼ˆWelfordç®—æ³•ï¼‰
        self.count += 1
        delta = features - self.mean
        self.mean += delta / self.count
        self.M2 += delta * (features - self.mean)
        
        # è®¡ç®—æ ‡å‡†å·®
        if self.count > 1:
            std = np.sqrt(self.M2 / (self.count - 1))
        else:
            std = np.ones(self.n_features)
            
        # æ ‡å‡†åŒ–
        normalized = (features - self.mean) / (std + 1e-8)
        
        # æˆªæ–­åˆ°[-3, 3]
        normalized = np.clip(normalized, -3, 3)
        
        # æ—¥å¿—
        logger.debug("ğŸ“Š æ ‡å‡†åŒ–ç‰¹å¾:")
        for name, value in zip(self.feature_names, normalized):
            logger.debug(f"  {name}: {value:.3f}")
            
        return normalized
    
    def extract_features(self, metrics: Dict[str, float]) -> np.ndarray:
        """é€‚é…å™¨æ–¹æ³•ï¼Œç”¨äºä¸main.pyçš„è°ƒç”¨æ–¹å¼å…¼å®¹"""
        # å°†åˆå¹¶çš„metricsåˆ†ç¦»ä¸ºgaugeå’Œcounter
        gauge_metrics = {}
        counter_deltas = {}
        
        # åˆ†ç¦»ä¸åŒç±»å‹çš„æŒ‡æ ‡
        for key, value in metrics.items():
            if key.endswith('_delta'):
                counter_deltas[key] = value
            elif key.startswith('vllm:'):
                gauge_metrics[key] = value
        
        # è°ƒç”¨åŸå§‹çš„extractæ–¹æ³•è·å–åŸå§‹ç‰¹å¾
        raw_features = self.extract(gauge_metrics, counter_deltas)
        
        # åº”ç”¨åœ¨çº¿æ ‡å‡†åŒ–
        normalized_features = self.normalize(raw_features)
        
        return normalized_features