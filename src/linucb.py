import numpy as np
import pickle
import json
from pathlib import Path
from typing import Optional
from datetime import datetime
from .logger import setup_logger

logger = setup_logger(__name__)

class LinUCB:
    """
    Hybrid/Global LinUCB - æ‰€æœ‰åŠ¨ä½œå…±äº«ä¸€å¥—å‚æ•°
    ä½¿ç”¨one-hotç¼–ç æ‹¼æ¥åŠ¨ä½œä¿¡æ¯
    """
    def __init__(self, n_features: int, n_actions: int, alpha: float = 3.0,
                 lambda_reg: float = 0.1,
                 model_dir: str = "data/models", auto_load: bool = True):
        self.n_features = n_features   # ç¯å¢ƒç‰¹å¾ç»´åº¦
        self.n_actions = n_actions     # åŠ¨ä½œæ•°é‡
        self.d = n_features + n_actions  # æ€»ç»´åº¦ = ç¯å¢ƒç‰¹å¾ + one-hotåŠ¨ä½œ
        self.alpha = alpha
        self.initial_alpha = alpha
        self.lambda_reg = lambda_reg
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(parents=True, exist_ok=True)

        # å…¨å±€å…±äº«çš„Aå’ŒbçŸ©é˜µ
        self.A = self.lambda_reg * np.eye(self.d, dtype=np.float32)
        self.b = np.zeros(self.d, dtype=np.float32)

        # ç»Ÿè®¡ä¿¡æ¯
        self.action_counts = [0] * n_actions
        self.total_rounds = 0
        self.total_reward = 0.0
        self.reward_history = []
        self.last_action = None  # è®°å½•ä¸Šä¸€æ¬¡çš„åŠ¨ä½œï¼ˆç”¨äºè®¡ç®—åˆ‡æ¢æˆæœ¬ï¼‰

        # ç”¨äºå†·å¯åŠ¨çš„éšæœºæ’åˆ—
        self._cold_start_permutation = None

        # æ¨¡å‹å…ƒæ•°æ®
        self.metadata = {
            'created_at': datetime.now().isoformat(),
            'updated_at': datetime.now().isoformat(),
            'version': '3.0-hybrid'
        }

        logger.info(f"ğŸ¤– åˆå§‹åŒ–Hybrid LinUCB:")
        logger.info(f"   ç¯å¢ƒç‰¹å¾: {n_features}ç»´")
        logger.info(f"   åŠ¨ä½œæ•°é‡: {n_actions}ä¸ª")
        logger.info(f"   æ€»ç»´åº¦: {self.d}ç»´")
        logger.info(f"   åˆå§‹Î±: {alpha}")
        logger.info(f"   æ­£åˆ™åŒ–Î»: {lambda_reg}")

        if auto_load:
            self.load_model()
        else:
            logger.info("ğŸ†• è·³è¿‡æ¨¡å‹åŠ è½½ï¼Œä»å…¨æ–°çŠ¶æ€å¼€å§‹")

    def _create_context_with_action(self, base_features: np.ndarray, action: int) -> np.ndarray:
        """å°†ç¯å¢ƒç‰¹å¾å’ŒåŠ¨ä½œone-hotç¼–ç æ‹¼æ¥"""
        # One-hotç¼–ç åŠ¨ä½œ
        action_one_hot = np.zeros(self.n_actions, dtype=np.float32)
        action_one_hot[action] = 1.0
        
        # æ‹¼æ¥
        context = np.concatenate([base_features, action_one_hot])
        
        assert context.shape[0] == self.d, \
            f"æ‹¼æ¥åç»´åº¦é”™è¯¯: {context.shape[0]} vs {self.d}"
        
        return context

    def select_action(self, base_features: np.ndarray) -> int:
        """é€‰æ‹©åŠ¨ä½œ - ä½¿ç”¨UCBç­–ç•¥"""
        # å†·å¯åŠ¨é˜¶æ®µï¼šå‰n_actions*3è½®ï¼Œéšæœºé¡ºåºæ¢ç´¢æ¯ä¸ªåŠ¨ä½œ
        explore_rounds = self.n_actions * 3
        if self.total_rounds < explore_rounds:
            # åˆå§‹åŒ–éšæœºæ’åˆ—
            if self._cold_start_permutation is None or len(self._cold_start_permutation) < explore_rounds:
                # ç”Ÿæˆ3è½®å®Œæ•´çš„éšæœºæ’åˆ—
                self._cold_start_permutation = []
                for _ in range(3):
                    self._cold_start_permutation.extend(np.random.permutation(self.n_actions))
            
            selected = self._cold_start_permutation[self.total_rounds]
            logger.info(f"ğŸ² [å†·å¯åŠ¨æ¢ç´¢] è½®æ¬¡ {self.total_rounds + 1}/{explore_rounds}, é€‰æ‹©åŠ¨ä½œ {selected}")
            return selected

        # è®¡ç®—å½“å‰å‚æ•°ä¼°è®¡
        try:
            theta = np.linalg.solve(self.A, self.b)
        except np.linalg.LinAlgError:
            logger.warning("âš ï¸ AçŸ©é˜µå¥‡å¼‚ï¼Œä½¿ç”¨ä¼ªé€†")
            theta = np.linalg.pinv(self.A) @ self.b

        # è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„UCBå€¼
        ucb_values = []
        for action in range(self.n_actions):
            # æ„é€ å®Œæ•´ç‰¹å¾
            x = self._create_context_with_action(base_features, action)
            
            # é¢„æµ‹å€¼
            pred = theta.dot(x)
            
            # ç½®ä¿¡åŒºé—´ï¼ˆä½¿ç”¨Choleskyåˆ†è§£åŠ é€Ÿï¼‰
            try:
                L = np.linalg.cholesky(self.A)
                v = np.linalg.solve(L, x)
                confidence = self.alpha * np.sqrt(np.dot(v, v))
            except np.linalg.LinAlgError:
                # å¦‚æœCholeskyåˆ†è§£å¤±è´¥ï¼Œä½¿ç”¨æ ‡å‡†æ–¹æ³•
                A_inv_x = np.linalg.solve(self.A, x)
                confidence = self.alpha * np.sqrt(x.dot(A_inv_x))
            
            ucb = pred + confidence
            ucb_values.append(ucb)
            
            logger.debug(f"  åŠ¨ä½œ{action}: pred={pred:.3f}, conf={confidence:.3f}, UCB={ucb:.3f}")

        # é€‰æ‹©UCBæœ€å¤§çš„åŠ¨ä½œï¼ˆæœ‰å¹¶åˆ—æ—¶éšæœºé€‰æ‹©ï¼‰
        max_ucb = max(ucb_values)
        candidates = [i for i, v in enumerate(ucb_values) if abs(v - max_ucb) < 1e-9]
        selected = np.random.choice(candidates)
        
        logger.info(f"ğŸ¯ é€‰æ‹©åŠ¨ä½œ {selected}, UCBå€¼: {ucb_values[selected]:.3f}")
        
        return selected

    def update(self, action: int, base_features: np.ndarray, reward: float):
        """æ›´æ–°æ¨¡å‹å‚æ•°"""
        # æ„é€ å®Œæ•´ç‰¹å¾
        x = self._create_context_with_action(base_features, action)
        
        # æ›´æ–°Aå’Œbï¼ˆä¸ä½¿ç”¨è¡°å‡ï¼‰
        self.A += np.outer(x, x)
        self.b += reward * x

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.action_counts[action] += 1
        self.total_rounds += 1
        self.total_reward += reward
        self.reward_history.append(reward)
        if len(self.reward_history) > 1000:
            self.reward_history.pop(0)
        
        self.last_action = action

        # Î±è¡°å‡ç­–ç•¥ï¼šå‰3*Kè½®ä¿æŒé«˜æ¢ç´¢ï¼Œä¹‹åç¼“æ…¢è¡°å‡
        if self.total_rounds <= self.n_actions * 3:
            # å†·å¯åŠ¨é˜¶æ®µä¿æŒé«˜æ¢ç´¢
            self.alpha = self.initial_alpha
        else:
            # ä½¿ç”¨æ›´ç¼“æ…¢çš„è¡°å‡
            rounds_after_explore = self.total_rounds - self.n_actions * 3
            self.alpha = max(0.5, self.initial_alpha / np.sqrt(1.0 + 0.01 * rounds_after_explore))
        
        logger.info(f"ğŸ“ˆ æ›´æ–°æ¨¡å‹: åŠ¨ä½œ={action}, å¥–åŠ±={reward:.6f}, Î±={self.alpha:.3f}, æ€»è½®æ¬¡={self.total_rounds}")

        # å®šæœŸä¿å­˜æ¨¡å‹
        if self.total_rounds % 10 == 0:
            self.save_model()

    def save_model(self, filename: Optional[str] = None):
        """ä¿å­˜æ¨¡å‹"""
        if filename is None:
            filename = f"linucb_hybrid_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
        
        filepath = self.model_dir / filename
        
        # æ›´æ–°å…ƒæ•°æ®
        self.metadata['updated_at'] = datetime.now().isoformat()
        self.metadata['total_rounds'] = self.total_rounds
        self.metadata['total_reward'] = self.total_reward
        self.metadata['avg_reward'] = self.total_reward / max(self.total_rounds, 1)
        
        # æ‰“åŒ…æ¨¡å‹æ•°æ®
        model_data = {
            'A': self.A.astype(np.float32),
            'b': self.b.astype(np.float32),
            'n_features': self.n_features,
            'n_actions': self.n_actions,
            'd': self.d,
            'alpha': self.alpha,
            'initial_alpha': self.initial_alpha,
            'lambda_reg': self.lambda_reg,
            'action_counts': self.action_counts,
            'total_rounds': self.total_rounds,
            'total_reward': self.total_reward,
            'reward_history': self.reward_history[-100:],  # åªä¿å­˜æœ€è¿‘100ä¸ª
            'last_action': self.last_action,
            'metadata': self.metadata
        }
        
        try:
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
            
            # åˆ›å»ºè½¯é“¾æ¥æŒ‡å‘æœ€æ–°æ¨¡å‹
            latest_path = self.model_dir / "latest_model.pkl"
            if latest_path.exists():
                latest_path.unlink()
            latest_path.symlink_to(filepath.name)
            
            # ä¿å­˜å…ƒæ•°æ®JSON
            meta_path = self.model_dir / "model_metadata.json"
            with open(meta_path, 'w') as f:
                json.dump(self.metadata, f, indent=2)
                
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")

    def load_model(self, filename: Optional[str] = None):
        """åŠ è½½æ¨¡å‹"""
        if filename is None:
            # å°è¯•åŠ è½½æœ€æ–°æ¨¡å‹
            latest_path = self.model_dir / "latest_model.pkl"
            if latest_path.exists():
                filepath = latest_path
            else:
                # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹æ–‡ä»¶
                model_files = list(self.model_dir.glob("linucb_hybrid_model_*.pkl"))
                if not model_files:
                    logger.info("ğŸ“‚ æ²¡æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œå°†ä»å¤´å¼€å§‹å­¦ä¹ ")
                    return False
                filepath = max(model_files, key=lambda p: p.stat().st_mtime)
        else:
            filepath = self.model_dir / filename
            
        if not filepath.exists():
            logger.warning(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {filepath}")
            return False
            
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
                
            # å…¼å®¹æ€§æ£€æŸ¥
            if model_data.get('version', '').startswith('3.0'):
                # æ–°ç‰ˆæœ¬æ¨¡å‹
                if (model_data['n_features'] != self.n_features or 
                    model_data['n_actions'] != self.n_actions):
                    logger.warning("âš ï¸ åŠ è½½çš„æ¨¡å‹å‚æ•°ä¸å½“å‰é…ç½®ä¸ç¬¦")
                    return False
            else:
                # æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œæ— æ³•å…¼å®¹
                logger.warning("âš ï¸ æ¨¡å‹ç‰ˆæœ¬è¿‡æ—§ï¼Œæ— æ³•åŠ è½½")
                return False
                
            # æ¢å¤å‚æ•°
            self.A = model_data['A']
            self.b = model_data['b']
            self.d = model_data['d']
            self.alpha = model_data['alpha']
            self.initial_alpha = model_data['initial_alpha']
            self.lambda_reg = model_data['lambda_reg']
            self.action_counts = model_data['action_counts']
            self.total_rounds = model_data['total_rounds']
            self.total_reward = model_data['total_reward']
            self.reward_history = model_data.get('reward_history', [])
            self.last_action = model_data.get('last_action', None)
            self.metadata = model_data['metadata']
            
            logger.info(f"âœ… æ¨¡å‹å·²åŠ è½½: {filepath}")
            logger.info(f"   æ€»è½®æ¬¡: {self.total_rounds}")
            logger.info(f"   å¹³å‡å¥–åŠ±: {self.total_reward / max(self.total_rounds, 1):.3f}")
            logger.info(f"   å½“å‰Î±: {self.alpha:.3f}")
            
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False

    def get_model_stats(self) -> dict:
        """è·å–æ¨¡å‹ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_rounds': self.total_rounds,
            'total_reward': self.total_reward,
            'avg_reward': self.total_reward / max(self.total_rounds, 1),
            'action_counts': self.action_counts,
            'current_alpha': self.alpha,
            'recent_rewards': self.reward_history[-20:] if self.reward_history else [],
            'last_action': self.last_action,
            'metadata': self.metadata
        }
        
        # è®¡ç®—åŠ¨ä½œåˆ†å¸ƒç†µ
        if sum(self.action_counts) > 0:
            probs = np.array(self.action_counts) / sum(self.action_counts)
            entropy = -np.sum(probs * np.log(probs + 1e-10))
            stats['action_entropy'] = entropy
            stats['effective_actions'] = np.exp(entropy)
        
        return stats